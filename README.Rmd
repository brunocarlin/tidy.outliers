---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# tidy.outliers

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://www.tidyverse.org/lifecycle/#experimental)
[![CRAN status](https://www.r-pkg.org/badges/version/tidy.outliers)](https://CRAN.R-project.org/package=tidy.outliers)
[![Codecov test coverage](https://codecov.io/gh/brunocarlin/tidy.outliers/branch/master/graph/badge.svg)](https://codecov.io/gh/brunocarlin/tidy.outliers?branch=master)
[![R-CMD-check](https://github.com/brunocarlin/tidy.outliers/workflows/R-CMD-check/badge.svg)](https://github.com/brunocarlin/tidy.outliers/actions)
<!-- badges: end -->

The goal of tidy.outliers is to allow for easy usage of many outliers removal methods, I currently plan to implement at least:


-   the method on the [lookout package](https://github.com/Sevvandi/lookout)
-   the maha distance method
-   some other simple outlier detenctions such as IQR



## Installation

You can not yet install the released version of tidy.outliers from [CRAN](https://CRAN.R-project.org) with:

``` r
#install.packages("tidy.outliers")
```

And the development version from GitHub with:

```{r eval=FALSE, include=TRUE}
# install.packages("devtools")
# devtools::install_github("brunocarlin/tidy.outliers")
```


## Example Mutation

This is a basic example which shows you how to solve a common problem:


### Load Libraries

```{r libraries, echo=TRUE, warning=FALSE,message=FALSE}
library(recipes)
library(tidy.outliers)
```


### Create recipe mutating probabilities

```{r recipe}
rec_obj <-
  recipe(mpg ~ ., data = mtcars) |>
  step_outliers_maha(all_numeric(), -all_outcomes()) |>
  step_outliers_lookout(all_numeric(),-contains(r"(.outliers)"),-all_outcomes()) |> 
  prep(mtcars)
```


### Investigate results

```{r juice}
juice(rec_obj) |> 
  select(contains(r"(.outliers)")) |> 
  arrange(.outliers_lookout |> desc())
```

```{r tidy}
tidy(rec_obj,number = 1)
```
```{r tidy2}
tidy(rec_obj,number = 2)
```

## Example filtering

### Create recipe filtering outliers

```{r recipe remove}
rec_obj2 <-
  recipe(mpg ~ ., data = mtcars) |>
  step_outliers_maha(all_numeric(), -all_outcomes()) |>
  step_outliers_lookout(all_numeric(),-contains(r"(.outliers)"),-all_outcomes()) |> 
  step_outliers_remove(contains(r"(.outliers)")) |> 
  prep(mtcars)
```


### Investigate results 

We can see that the mtcars dataset got reduced by our function but way less than if we used just one function on this case

```{r juice remove}
juice(rec_obj2) |> glimpse()
```
```{r}
mtcars |> glimpse()
```

And we can get which were the outliers and their probability

```{r tidy remove}
tidy(rec_obj2,number = 3) |> 
  arrange(aggregation_results |> desc())
```

## Integration with tidymodels

### Load tidymodels

```{r echo=TRUE, warning=FALSE,message=FALSE }
library(tidymodels)
```

### Get data
```{r}
data(ames)


data_split <- ames |>
  mutate(Sale_Price = log10(Sale_Price)) |>
  initial_split(strata = Sale_Price)
ames_train <- training(data_split)
ames_test  <- testing(data_split)
```


### create the recipe

```{r}
ames_rec <- 
  recipe(Sale_Price ~ Gr_Liv_Area + Longitude + Latitude, data = ames_train) |> 
  step_log(Gr_Liv_Area, base = 10) |> 
  step_ns(Longitude, deg_free = tune("long df")) |> 
  step_ns(Latitude,  deg_free = tune("lat df")) |> 
  step_outliers_maha(all_numeric(), -all_outcomes()) |>
  step_outliers_lookout(all_numeric(),-contains(r"(.outliers)"),-all_outcomes()) |> 
  step_outliers_remove(contains(r"(.outliers)"),probability_dropout = tune("dropout"),aggregation_function = tune("aggregation"))

```

### See the parameters

```{r}
parameters(ames_rec)
```

### There is already a function for dropouts implemented by dials

```{r}
ames_param <- 
  ames_rec |> 
  parameters() |> 
  update(
    `long df` = spline_degree(), 
    `lat df` = spline_degree(),
    dropout = dropout(range = c(0.75, 1)),
    aggregation = aggregation() |> value_set(c("mean","weighted_mean"))
  )
```

## Create weighted_mean func

```{r}
weighted_mean <- function(x) {
  x[[1]] * .75 + x[[2]] * .25
}
```


### Grid Search picks random points

```{r}
spline_grid <- grid_max_entropy(ames_param, size = 20)
spline_grid
```



### create a simple model

```{r}
lin_mod <-
  linear_reg() |>
  set_engine("lm")
```

### create a simple workflow   

```{r}

wf_tune <- workflow() |>
  add_recipe(ames_rec) |> 
  add_model(lin_mod)
```

### create training folds

```{r}
set.seed(2453)
cv_splits <- vfold_cv(ames_train, v = 5, strata = Sale_Price)
```


### Tune the grid

```{r calculate grid, echo=TRUE, warning=FALSE,message=FALSE}
ames_res <- tune_grid(wf_tune, resamples = cv_splits, grid = spline_grid)
```


```{r}

estimates <- collect_metrics(ames_res)

rmse_vals <- 
  estimates |> 
  dplyr::filter(.metric == "rmse") |> 
  arrange(mean)
rmse_vals
```

## Plot it

```{r}
autoplot(ames_res,metric = "rmse")
```
